{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae-gan.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pokerface1o1/vae-gan/blob/main/vae_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eefVPt_P_T8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "37556504-6b7f-43f9-9b91-0da98c985fd7"
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.1-cp36-cp36m-linux_x86_64.whl \n",
        "!pip3 install torchvision\n",
        "!pip3 install visdom\n",
        "!pip3 install pycrayon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.1 from http://download.pytorch.org/whl/cu80/torch-0.3.1-cp36-cp36m-linux_x86_64.whl\n",
            "  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.1-cp36-cp36m-linux_x86_64.whl (496.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 496.4MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.1)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-0.3.1\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.2.0-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "  Downloading Pillow-5.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.9MB)\n",
            "\u001b[K    99% |████████████████████████████████| 5.8MB 40.4MB/s eta 0:00:01"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 5.9MB 192kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch->torchvision)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.0.0 torchvision-0.2.0\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.1.7.tar.gz (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from visdom)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom)\n",
            "Collecting torchfile (from visdom)\n",
            "  Downloading torchfile-0.1.0.tar.gz\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom)\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Running setup.py bdist_wheel for visdom ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d3/45/ad/a5b60260d8e6122de5cbafd192ba55aa61d9b23c3b43aff34c\n",
            "  Running setup.py bdist_wheel for torchfile ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/27/0a/74/650124cbc320716ff2a6566e711a0c84f871647b33ffc868f6\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: torchfile, visdom\n",
            "Successfully installed torchfile-0.1.0 visdom-0.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZlQ-d9LAtvz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5375
        },
        "outputId": "b6c36459-23c5-453e-f1e9-a9b57ca0fc0c"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.legacy.nn as lnn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "#import visdom\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "\n",
        "#vis = visdom.Visdom()\n",
        "#vis.env = 'vae_dcgan'\n",
        "\n",
        "batch_size = 100\n",
        "nz = 100\n",
        "niter = 25\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "trainset = dset.MNIST(root='../data', train=True,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "testset = dset.MNIST(root='../data', train=False,\n",
        "                                     download=True,\n",
        "                                     transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                        shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "class Sampler(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sampler, self).__init__()\n",
        "        \n",
        "    def forward(self,input):\n",
        "        #mu = input[0]\n",
        "        #logvar = input[1]\n",
        "        mu, logvar = torch.chunk(input, 2, dim=1)\n",
        "        \n",
        "        std = logvar.mul(0.5).exp_() #calculate the STDEV\n",
        "        #if opt.cuda:\n",
        "        eps = torch.cuda.FloatTensor(std.size()).normal_() #random normalized noise\n",
        "        #else:\n",
        "        #eps = torch.FloatTensor(std.size()).normal_() #random normalized noise\n",
        "        eps = Variable(eps)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=nz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.Linear(7*7*32, h_dim),\n",
        "            nn.Linear(h_dim, z_dim*2))  # 2 for mean and variance.\n",
        "    \"\"\"    \n",
        "    def reparametrize(self, mu, log_var):\n",
        "        \\\"\"\"z = mean + eps * sigma where eps is sampled from N(0, 1).\\\"\"\"\n",
        "        eps = Variable(torch.randn(mu.size(0), mu.size(1)))\n",
        "        z = mu + eps * torch.exp(log_var/2)  # 2 for convert var to std\n",
        "        return z\n",
        "    \"\"\"\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.encoder1(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.encoder2(out)\n",
        "          # mean and log variance.\n",
        "        #z = self.reparametrize(mu, log_var)    \n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=nz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.Linear(z_dim, h_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(h_dim, 7*7*32))\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=4, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=4, padding=1, stride=2),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Tanh())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.decoder1(x)\n",
        "        out = out.view(out.size(0), 32, 7, 7)\n",
        "        out = self.decoder2(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_size=784, h_dim=400, z_dim=nz):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.sampler = Sampler()\n",
        "        self.decoder = Decoder()\n",
        "                     \n",
        "    def forward(self,x):\n",
        "        out = self.encoder(x)\n",
        "        out = self.sampler(out)\n",
        "        out = self.decoder(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def make_cuda(self):\n",
        "        self.encoder.cuda()\n",
        "        self.sampler.cuda()\n",
        "        self.decoder.cuda()\n",
        "\n",
        "class ImprovedGAN_Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, featmap_dim=512, n_channel=1, use_gpu=True,\n",
        "                 n_B=128, n_C=16):\n",
        "        \"\"\"\n",
        "        Minibatch discrimination: learn a tensor to encode side information\n",
        "        from other examples in the same minibatch.\n",
        "        \"\"\"\n",
        "        super(ImprovedGAN_Discriminator, self).__init__()\n",
        "        self.use_gpu = use_gpu\n",
        "        self.n_B = n_B\n",
        "        self.n_C = n_C\n",
        "        self.featmap_dim = featmap_dim\n",
        "\n",
        "        self.conv1 = nn.Conv2d(n_channel, int(featmap_dim / 4), 5,\n",
        "                               stride=2, padding=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(int(featmap_dim / 4), int(featmap_dim / 2), 5,\n",
        "                               stride=2, padding=2)\n",
        "        self.BN2 = nn.BatchNorm2d(int(featmap_dim / 2))\n",
        "\n",
        "        self.conv3 = nn.Conv2d(int(featmap_dim / 2), featmap_dim, 5,\n",
        "                               stride=2, padding=2)\n",
        "        self.BN3 = nn.BatchNorm2d(featmap_dim)\n",
        "\n",
        "        T_ten_init = torch.randn(featmap_dim * 4 * 4, n_B * n_C) * 0.1\n",
        "        self.T_tensor = nn.Parameter(T_ten_init, requires_grad=True)\n",
        "        self.fc = nn.Linear(featmap_dim * 4 * 4 + n_B, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Architecture is similar to DCGANs\n",
        "        Add minibatch discrimination => Improved GAN.\n",
        "        \"\"\"\n",
        "        x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.BN2(self.conv2(x)), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.BN3(self.conv3(x)), negative_slope=0.2)\n",
        "        x = x.view(-1, self.featmap_dim * 4 * 4)\n",
        "\n",
        "        # #### Minibatch Discrimination ###\n",
        "        T_tensor = self.T_tensor\n",
        "        if self.use_gpu:\n",
        "            T_tensor = T_tensor.cuda()\n",
        "\n",
        "        Ms = x.mm(T_tensor)\n",
        "        Ms = Ms.view(-1, self.n_B, self.n_C)\n",
        "\n",
        "        out_tensor = []\n",
        "        for i in range(Ms.size()[0]):\n",
        "\n",
        "            out_i = None\n",
        "            for j in range(Ms.size()[0]):\n",
        "                o_i = torch.sum(torch.abs(Ms[i, :, :] - Ms[j, :, :]), 1)\n",
        "                o_i = torch.exp(-o_i)\n",
        "                if out_i is None:\n",
        "                    out_i = o_i\n",
        "                else:\n",
        "                    out_i = out_i + o_i\n",
        "\n",
        "            out_tensor.append(out_i)\n",
        "\n",
        "        out_T = torch.cat(tuple(out_tensor)).view(Ms.size()[0], self.n_B)\n",
        "        x = torch.cat((x, out_T), 1)\n",
        "        # #### Minibatch Discrimination ###\n",
        "\n",
        "        x = F.sigmoid(self.fc(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    \n",
        "\n",
        "netG = VAE()\n",
        "netD = ImprovedGAN_Discriminator()\n",
        "criterion = nn.BCELoss()\n",
        "MSECriterion = nn.MSELoss()\n",
        "\n",
        "input = torch.FloatTensor(batch_size, 1, 28, 28)\n",
        "noise = torch.FloatTensor(2*((torch.randn(batch_size, nz).bernoulli_(0.5))-0.5))\n",
        "fixed_noise = torch.FloatTensor(2*((torch.randn(batch_size, nz).bernoulli_(0.5))-0.5))\n",
        "label = torch.FloatTensor(batch_size)\n",
        "#real_label = 1\n",
        "#fake_label = 0\n",
        "\n",
        "print(\"CU:\" + str(torch.cuda.is_available()))\n",
        "netD.cuda()\n",
        "netG.make_cuda()\n",
        "criterion.cuda()\n",
        "MSECriterion.cuda()\n",
        "input, label = input.cuda(), label.cuda()\n",
        "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
        "\n",
        "input = Variable(input)\n",
        "label = Variable(label)\n",
        "noise = Variable(noise)\n",
        "fixed_noise = Variable(fixed_noise)\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "gen_win = None\n",
        "rec_win = None\n",
        "\n",
        "for epoch in range(niter):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        netD.zero_grad()\n",
        "        real_cpu, _ = data\n",
        "        batch_size = real_cpu.size(0)\n",
        "        input.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
        "        real_label = random.uniform(0.6,1.2)\n",
        "        label.data.resize_(real_cpu.size(0)).fill_(real_label)\n",
        "        \n",
        "        output = netD(input)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.data.mean()\n",
        "        noise.data.resize_(batch_size, nz)\n",
        "        noise.data.normal_(0, 1)\n",
        "        gen = netG.decoder(noise)\n",
        "        #print(gen.size())\n",
        "        #gen_win = gen.data.numpy()\n",
        "        if i%50==0:\n",
        "            vutils.save_image(gen.data*0.5+0.5, './noise_'+str(epoch)+'_'+str(i)+'.png')\n",
        "            #files.download('./noise_'+str(epoch)+'_'+str(i)+'.png')\n",
        "        #gen_win = vis.image(gen.data[0].cpu()*0.5+0.5,win = gen_win)\n",
        "        #print(gen.size())\n",
        "        fake_label = random.uniform(0.0,0.3)\n",
        "        label.data.fill_(fake_label)\n",
        "        output = netD(gen.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.data.mean()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "        \n",
        "        #print(input.size())\n",
        "        encoded = netG.encoder(input)\n",
        "        mu, logvar = torch.chunk(encoded, 2, dim=1)\n",
        "        \n",
        "        KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
        "        KLD = torch.sum(KLD_element).mul_(-0.5)\n",
        "        \n",
        "        sampled = netG.sampler(encoded)\n",
        "        rec = netG.decoder(sampled)\n",
        "        #rec_win = rec.data.numpy()\n",
        "        if i%50==0:\n",
        "            vutils.save_image(rec.data*0.5+0.5, './sample_'+str(epoch)+'_'+str(i)+'.png')\n",
        "            #files.download('./sample_'+str(epoch)+'_'+str(i)+'.png')\n",
        "        #rec_win = vis.image(rec.data[0].cpu()*0.5+0.5,win = rec_win)\n",
        "        \n",
        "        MSEerr = MSECriterion(rec,input)\n",
        "        \n",
        "        VAEerr = KLD + MSEerr\n",
        "        VAEerr.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        netG.zero_grad()\n",
        "        real_label = random.uniform(0.6,1.2)\n",
        "        label.data.fill_(real_label)  # fake labels are real for generator cost\n",
        "\n",
        "        rec = netG(input) # this tensor is freed from mem at this point\n",
        "        output = netD(rec)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.data.mean()\n",
        "        optimizerG.step()\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_VAE: %.4f Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "              % (epoch, niter, i, len(trainloader),\n",
        "                 VAEerr.data[0], errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        if(epoch!=0):\n",
        "          torch.save(netG, './g' )\n",
        "          torch.save(netD, './d')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CU:True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/25][0/600] Loss_VAE: 387.4487 Loss_D: 1.4017 Loss_G: 1.4593 D(x): 0.4855 D(G(z)): 0.4878 / 0.2614\n",
            "[0/25][1/600] Loss_VAE: 166.8496 Loss_D: 0.1268 Loss_G: 0.9465 D(x): 0.9713 D(G(z)): 0.3710 / 0.3987\n",
            "[0/25][2/600] Loss_VAE: 117.1510 Loss_D: 3.1381 Loss_G: 1.8336 D(x): 0.9959 D(G(z)): 0.6110 / 0.1289\n",
            "[0/25][3/600] Loss_VAE: 97.5845 Loss_D: 1.1811 Loss_G: 2.7654 D(x): 0.9723 D(G(z)): 0.2891 / 0.0746\n",
            "[0/25][4/600] Loss_VAE: 97.7590 Loss_D: 0.9923 Loss_G: 1.1530 D(x): 0.7546 D(G(z)): 0.2022 / 0.3350\n",
            "[0/25][5/600] Loss_VAE: 98.0717 Loss_D: 1.2568 Loss_G: 1.6834 D(x): 0.7348 D(G(z)): 0.5652 / 0.1185\n",
            "[0/25][6/600] Loss_VAE: 87.1173 Loss_D: 0.6899 Loss_G: 1.8499 D(x): 0.7065 D(G(z)): 0.2019 / 0.1478\n",
            "[0/25][7/600] Loss_VAE: 79.0588 Loss_D: -0.1132 Loss_G: 2.1478 D(x): 0.9586 D(G(z)): 0.2651 / 0.0488\n",
            "[0/25][8/600] Loss_VAE: 75.6929 Loss_D: 1.2243 Loss_G: 1.1602 D(x): 0.9888 D(G(z)): 0.1166 / 0.2772\n",
            "[0/25][9/600] Loss_VAE: 69.6176 Loss_D: 0.6941 Loss_G: 3.7507 D(x): 0.9907 D(G(z)): 0.5148 / 0.0211\n",
            "[0/25][10/600] Loss_VAE: 67.2056 Loss_D: 1.3253 Loss_G: 2.3381 D(x): 0.9734 D(G(z)): 0.0585 / 0.0471\n",
            "[0/25][11/600] Loss_VAE: 70.8999 Loss_D: 1.5927 Loss_G: 1.1910 D(x): 0.9082 D(G(z)): 0.1080 / 0.1678\n",
            "[0/25][12/600] Loss_VAE: 75.4922 Loss_D: 1.5062 Loss_G: 0.5721 D(x): 0.3853 D(G(z)): 0.2870 / 0.5716\n",
            "[0/25][13/600] Loss_VAE: 91.0434 Loss_D: 0.6299 Loss_G: 3.3797 D(x): 0.9270 D(G(z)): 0.6508 / 0.0484\n",
            "[0/25][14/600] Loss_VAE: 95.3868 Loss_D: 1.8555 Loss_G: 2.4621 D(x): 0.9667 D(G(z)): 0.0801 / 0.1065\n",
            "[0/25][15/600] Loss_VAE: 80.9632 Loss_D: 0.3713 Loss_G: 0.4832 D(x): 0.9394 D(G(z)): 0.2099 / 0.6000\n",
            "[0/25][16/600] Loss_VAE: 52.0805 Loss_D: 1.5780 Loss_G: 2.8343 D(x): 0.9639 D(G(z)): 0.7050 / 0.0234\n",
            "[0/25][17/600] Loss_VAE: 40.6001 Loss_D: 0.2475 Loss_G: 2.8524 D(x): 0.8494 D(G(z)): 0.0283 / 0.0214\n",
            "[0/25][18/600] Loss_VAE: 34.7569 Loss_D: 1.0220 Loss_G: 3.5645 D(x): 0.9106 D(G(z)): 0.0258 / 0.0482\n",
            "[0/25][19/600] Loss_VAE: 32.2907 Loss_D: 0.6337 Loss_G: 2.6163 D(x): 0.9278 D(G(z)): 0.0610 / 0.0476\n",
            "[0/25][20/600] Loss_VAE: 34.8839 Loss_D: -0.1692 Loss_G: 2.3799 D(x): 0.8955 D(G(z)): 0.0616 / 0.0695\n",
            "[0/25][21/600] Loss_VAE: 39.4245 Loss_D: 1.1664 Loss_G: 1.5000 D(x): 0.9360 D(G(z)): 0.0830 / 0.1555\n",
            "[0/25][22/600] Loss_VAE: 47.4626 Loss_D: 0.5733 Loss_G: 2.4610 D(x): 0.9401 D(G(z)): 0.1895 / 0.0897\n",
            "[0/25][23/600] Loss_VAE: 56.5749 Loss_D: 1.4282 Loss_G: 1.3851 D(x): 0.8988 D(G(z)): 0.1099 / 0.2012\n",
            "[0/25][24/600] Loss_VAE: 60.8271 Loss_D: 0.6813 Loss_G: 1.6164 D(x): 0.8448 D(G(z)): 0.2348 / 0.1049\n",
            "[0/25][25/600] Loss_VAE: 53.5113 Loss_D: 0.9354 Loss_G: 1.1015 D(x): 0.7793 D(G(z)): 0.1158 / 0.3053\n",
            "[0/25][26/600] Loss_VAE: 40.3562 Loss_D: 0.8859 Loss_G: 3.0121 D(x): 0.8863 D(G(z)): 0.3280 / 0.0246\n",
            "[0/25][27/600] Loss_VAE: 31.4922 Loss_D: 0.8297 Loss_G: 4.9263 D(x): 0.7413 D(G(z)): 0.0270 / 0.0072\n",
            "[0/25][28/600] Loss_VAE: 29.1898 Loss_D: 2.1688 Loss_G: 1.8542 D(x): 0.5102 D(G(z)): 0.0079 / 0.1498\n",
            "[0/25][29/600] Loss_VAE: 29.6700 Loss_D: 0.4587 Loss_G: 0.7013 D(x): 0.9361 D(G(z)): 0.1660 / 0.4966\n",
            "[0/25][30/600] Loss_VAE: 33.7907 Loss_D: 0.3426 Loss_G: 2.7172 D(x): 0.9789 D(G(z)): 0.5235 / 0.1001\n",
            "[0/25][31/600] Loss_VAE: 36.9772 Loss_D: 1.6385 Loss_G: 4.1963 D(x): 0.9764 D(G(z)): 0.1092 / 0.0274\n",
            "[0/25][32/600] Loss_VAE: 40.3942 Loss_D: 0.5482 Loss_G: 2.1781 D(x): 0.9249 D(G(z)): 0.0299 / 0.0894\n",
            "[0/25][33/600] Loss_VAE: 38.4039 Loss_D: 1.3419 Loss_G: 2.0589 D(x): 0.9557 D(G(z)): 0.0997 / 0.1362\n",
            "[0/25][34/600] Loss_VAE: 33.3058 Loss_D: 0.7331 Loss_G: 1.2876 D(x): 0.9360 D(G(z)): 0.1485 / 0.2604\n",
            "[0/25][35/600] Loss_VAE: 28.2820 Loss_D: 0.9721 Loss_G: 1.8771 D(x): 0.9417 D(G(z)): 0.2799 / 0.1370\n",
            "[0/25][36/600] Loss_VAE: 27.0439 Loss_D: 0.2115 Loss_G: 3.2105 D(x): 0.8845 D(G(z)): 0.1426 / 0.0648\n",
            "[0/25][37/600] Loss_VAE: 24.7923 Loss_D: 0.6397 Loss_G: 3.4371 D(x): 0.8652 D(G(z)): 0.0673 / 0.0336\n",
            "[0/25][38/600] Loss_VAE: 24.5262 Loss_D: 0.6953 Loss_G: 2.4540 D(x): 0.8301 D(G(z)): 0.0349 / 0.0181\n",
            "[0/25][39/600] Loss_VAE: 23.6542 Loss_D: 1.8035 Loss_G: 2.8103 D(x): 0.6623 D(G(z)): 0.0190 / 0.0532\n",
            "[0/25][40/600] Loss_VAE: 24.4203 Loss_D: 1.2387 Loss_G: 1.1797 D(x): 0.7317 D(G(z)): 0.0551 / 0.2010\n",
            "[0/25][41/600] Loss_VAE: 26.8997 Loss_D: 0.7222 Loss_G: 0.9930 D(x): 0.8420 D(G(z)): 0.2084 / 0.3737\n",
            "[0/25][42/600] Loss_VAE: 27.5447 Loss_D: 1.5129 Loss_G: 1.7586 D(x): 0.8950 D(G(z)): 0.3812 / 0.1837\n",
            "[0/25][43/600] Loss_VAE: 31.3767 Loss_D: 0.9254 Loss_G: 3.3157 D(x): 0.7943 D(G(z)): 0.1899 / 0.0601\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/25][44/600] Loss_VAE: 33.4761 Loss_D: 1.0257 Loss_G: 1.7302 D(x): 0.5047 D(G(z)): 0.0614 / 0.0968\n",
            "[0/25][45/600] Loss_VAE: 34.2090 Loss_D: 0.9235 Loss_G: 2.0127 D(x): 0.8025 D(G(z)): 0.0998 / 0.1115\n",
            "[0/25][46/600] Loss_VAE: 31.7513 Loss_D: 0.8875 Loss_G: 1.7046 D(x): 0.8627 D(G(z)): 0.1144 / 0.0778\n",
            "[0/25][47/600] Loss_VAE: 26.6526 Loss_D: 0.8464 Loss_G: 2.2434 D(x): 0.8218 D(G(z)): 0.0802 / 0.0565\n",
            "[0/25][48/600] Loss_VAE: 23.0641 Loss_D: 1.4014 Loss_G: 1.8391 D(x): 0.7460 D(G(z)): 0.0576 / 0.1071\n",
            "[0/25][49/600] Loss_VAE: 20.0070 Loss_D: 0.9478 Loss_G: 1.7828 D(x): 0.7585 D(G(z)): 0.1093 / 0.1685\n",
            "[0/25][50/600] Loss_VAE: 19.8107 Loss_D: 0.6274 Loss_G: 0.9711 D(x): 0.7795 D(G(z)): 0.1730 / 0.3763\n",
            "[0/25][51/600] Loss_VAE: 18.5235 Loss_D: 1.4133 Loss_G: 2.0229 D(x): 0.9033 D(G(z)): 0.3836 / 0.1368\n",
            "[0/25][52/600] Loss_VAE: 20.0389 Loss_D: 1.2321 Loss_G: 2.2211 D(x): 0.8337 D(G(z)): 0.1391 / 0.0929\n",
            "[0/25][53/600] Loss_VAE: 21.7364 Loss_D: 0.9136 Loss_G: 2.5734 D(x): 0.6978 D(G(z)): 0.0943 / 0.0901\n",
            "[0/25][54/600] Loss_VAE: 25.4008 Loss_D: 0.7609 Loss_G: 1.9287 D(x): 0.6863 D(G(z)): 0.0913 / 0.0703\n",
            "[0/25][55/600] Loss_VAE: 26.9716 Loss_D: 1.1406 Loss_G: 1.8328 D(x): 0.6154 D(G(z)): 0.0707 / 0.1397\n",
            "[0/25][56/600] Loss_VAE: 27.5147 Loss_D: 1.3624 Loss_G: 1.0110 D(x): 0.7848 D(G(z)): 0.1410 / 0.2443\n",
            "[0/25][57/600] Loss_VAE: 25.5043 Loss_D: 1.1910 Loss_G: 1.5595 D(x): 0.8197 D(G(z)): 0.2453 / 0.2212\n",
            "[0/25][58/600] Loss_VAE: 23.2456 Loss_D: 0.8567 Loss_G: 1.0841 D(x): 0.7564 D(G(z)): 0.2229 / 0.2163\n",
            "[0/25][59/600] Loss_VAE: 19.8603 Loss_D: 0.7489 Loss_G: 1.7331 D(x): 0.7975 D(G(z)): 0.2180 / 0.1856\n",
            "[0/25][60/600] Loss_VAE: 19.3932 Loss_D: 0.6837 Loss_G: 2.2454 D(x): 0.8291 D(G(z)): 0.1884 / 0.1344\n",
            "[0/25][61/600] Loss_VAE: 18.0122 Loss_D: 0.1661 Loss_G: 1.7492 D(x): 0.8539 D(G(z)): 0.1367 / 0.0986\n",
            "[0/25][62/600] Loss_VAE: 16.3635 Loss_D: 0.5504 Loss_G: 1.5783 D(x): 0.9173 D(G(z)): 0.0998 / 0.1388\n",
            "[0/25][63/600] Loss_VAE: 15.9864 Loss_D: -0.3479 Loss_G: 1.8996 D(x): 0.9559 D(G(z)): 0.1430 / 0.1397\n",
            "[0/25][64/600] Loss_VAE: 15.7854 Loss_D: 0.6652 Loss_G: 1.0734 D(x): 0.9791 D(G(z)): 0.1489 / 0.2242\n",
            "[0/25][65/600] Loss_VAE: 14.8669 Loss_D: -0.2435 Loss_G: 1.3666 D(x): 0.9881 D(G(z)): 0.2388 / 0.2691\n",
            "[0/25][66/600] Loss_VAE: 14.6332 Loss_D: 1.4472 Loss_G: 2.0260 D(x): 0.9946 D(G(z)): 0.2970 / 0.1632\n",
            "[0/25][67/600] Loss_VAE: 15.6699 Loss_D: 1.9737 Loss_G: 1.3193 D(x): 0.9901 D(G(z)): 0.1833 / 0.1942\n",
            "[0/25][68/600] Loss_VAE: 14.9949 Loss_D: 0.9053 Loss_G: 2.7643 D(x): 0.9802 D(G(z)): 0.2059 / 0.0964\n",
            "[0/25][69/600] Loss_VAE: 15.7219 Loss_D: 0.4602 Loss_G: 1.8567 D(x): 0.9521 D(G(z)): 0.1022 / 0.1618\n",
            "[0/25][70/600] Loss_VAE: 15.3697 Loss_D: 0.6723 Loss_G: 1.8801 D(x): 0.9536 D(G(z)): 0.1712 / 0.1437\n",
            "[0/25][71/600] Loss_VAE: 15.7973 Loss_D: 0.2401 Loss_G: 0.7950 D(x): 0.9457 D(G(z)): 0.1517 / 0.4553\n",
            "[0/25][72/600] Loss_VAE: 15.9091 Loss_D: 1.4693 Loss_G: 2.4434 D(x): 0.9731 D(G(z)): 0.4689 / 0.0640\n",
            "[0/25][73/600] Loss_VAE: 16.6993 Loss_D: 1.2072 Loss_G: 3.1728 D(x): 0.9358 D(G(z)): 0.0653 / 0.0203\n",
            "[0/25][74/600] Loss_VAE: 17.0381 Loss_D: 1.0769 Loss_G: 2.6333 D(x): 0.7274 D(G(z)): 0.0209 / 0.0331\n",
            "[0/25][75/600] Loss_VAE: 17.7607 Loss_D: 0.3731 Loss_G: 3.2275 D(x): 0.7778 D(G(z)): 0.0343 / 0.0556\n",
            "[0/25][76/600] Loss_VAE: 19.6519 Loss_D: 0.7825 Loss_G: 1.6351 D(x): 0.8632 D(G(z)): 0.0585 / 0.1526\n",
            "[0/25][77/600] Loss_VAE: 20.4726 Loss_D: 0.5897 Loss_G: 0.9025 D(x): 0.9399 D(G(z)): 0.1582 / 0.3962\n",
            "[0/25][78/600] Loss_VAE: 20.4709 Loss_D: 1.4394 Loss_G: 1.5975 D(x): 0.9736 D(G(z)): 0.4021 / 0.2304\n",
            "[0/25][79/600] Loss_VAE: 16.6451 Loss_D: 0.4296 Loss_G: 1.6718 D(x): 0.9571 D(G(z)): 0.2327 / 0.1223\n",
            "[0/25][80/600] Loss_VAE: 13.4441 Loss_D: 0.5617 Loss_G: 2.4673 D(x): 0.9422 D(G(z)): 0.1238 / 0.1165\n",
            "[0/25][81/600] Loss_VAE: 11.6765 Loss_D: 1.2003 Loss_G: 1.8577 D(x): 0.9483 D(G(z)): 0.1184 / 0.1026\n",
            "[0/25][82/600] Loss_VAE: 10.7304 Loss_D: 0.4488 Loss_G: 1.5498 D(x): 0.9150 D(G(z)): 0.1037 / 0.1085\n",
            "[0/25][83/600] Loss_VAE: 10.4439 Loss_D: 0.3305 Loss_G: 1.5653 D(x): 0.9188 D(G(z)): 0.1111 / 0.0910\n",
            "[0/25][84/600] Loss_VAE: 11.7470 Loss_D: 1.4752 Loss_G: 2.2186 D(x): 0.9158 D(G(z)): 0.0942 / 0.1180\n",
            "[0/25][85/600] Loss_VAE: 13.4623 Loss_D: 0.1722 Loss_G: 1.3209 D(x): 0.9026 D(G(z)): 0.1195 / 0.1687\n",
            "[0/25][86/600] Loss_VAE: 17.7425 Loss_D: 1.1459 Loss_G: 1.8166 D(x): 0.9217 D(G(z)): 0.1695 / 0.1992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/25][87/600] Loss_VAE: 25.1502 Loss_D: 0.5214 Loss_G: 1.6501 D(x): 0.9179 D(G(z)): 0.2024 / 0.1852\n",
            "[0/25][88/600] Loss_VAE: 34.3492 Loss_D: -0.1244 Loss_G: 1.9373 D(x): 0.9297 D(G(z)): 0.1906 / 0.1592\n",
            "[0/25][89/600] Loss_VAE: 41.3276 Loss_D: 0.7357 Loss_G: 2.5868 D(x): 0.9548 D(G(z)): 0.1646 / 0.0953\n",
            "[0/25][90/600] Loss_VAE: 34.5333 Loss_D: 0.8559 Loss_G: 2.0359 D(x): 0.9364 D(G(z)): 0.0957 / 0.1410\n",
            "[0/25][91/600] Loss_VAE: 20.2819 Loss_D: -0.2370 Loss_G: 1.0702 D(x): 0.9623 D(G(z)): 0.1438 / 0.2013\n",
            "[0/25][92/600] Loss_VAE: 12.3178 Loss_D: 1.3144 Loss_G: 1.2612 D(x): 0.9795 D(G(z)): 0.2091 / 0.1605\n",
            "[0/25][93/600] Loss_VAE: 10.8377 Loss_D: 0.2484 Loss_G: 1.8546 D(x): 0.9653 D(G(z)): 0.1650 / 0.1500\n",
            "[0/25][94/600] Loss_VAE: 11.6959 Loss_D: 0.6052 Loss_G: 2.5675 D(x): 0.9771 D(G(z)): 0.1514 / 0.0883\n",
            "[0/25][95/600] Loss_VAE: 11.6096 Loss_D: 0.8693 Loss_G: 3.2656 D(x): 0.9576 D(G(z)): 0.0879 / 0.0557\n",
            "[0/25][96/600] Loss_VAE: 11.7349 Loss_D: 0.8228 Loss_G: 2.1891 D(x): 0.9379 D(G(z)): 0.0558 / 0.0524\n",
            "[0/25][97/600] Loss_VAE: 12.5367 Loss_D: 0.2740 Loss_G: 2.2343 D(x): 0.9079 D(G(z)): 0.0524 / 0.0490\n",
            "[0/25][98/600] Loss_VAE: 12.9866 Loss_D: 0.0858 Loss_G: 3.1539 D(x): 0.8742 D(G(z)): 0.0486 / 0.0720\n",
            "[0/25][99/600] Loss_VAE: 13.1098 Loss_D: 0.5065 Loss_G: 2.2886 D(x): 0.9353 D(G(z)): 0.0760 / 0.1085\n",
            "[0/25][100/600] Loss_VAE: 12.4011 Loss_D: -0.0728 Loss_G: 1.3223 D(x): 0.9616 D(G(z)): 0.1238 / 0.1225\n",
            "[0/25][101/600] Loss_VAE: 12.0495 Loss_D: 1.4608 Loss_G: 1.9959 D(x): 0.9767 D(G(z)): 0.1330 / 0.0531\n",
            "[0/25][102/600] Loss_VAE: 11.5390 Loss_D: 1.3097 Loss_G: 2.7470 D(x): 0.9486 D(G(z)): 0.0586 / 0.0269\n",
            "[0/25][103/600] Loss_VAE: 10.6470 Loss_D: 1.3450 Loss_G: 1.7713 D(x): 0.8212 D(G(z)): 0.0314 / 0.0618\n",
            "[0/25][104/600] Loss_VAE: 9.8666 Loss_D: 0.6916 Loss_G: 1.9468 D(x): 0.8010 D(G(z)): 0.0638 / 0.1703\n",
            "[0/25][105/600] Loss_VAE: 9.7921 Loss_D: 0.2041 Loss_G: 1.0242 D(x): 0.9116 D(G(z)): 0.1877 / 0.2277\n",
            "[0/25][106/600] Loss_VAE: 9.9545 Loss_D: 0.1390 Loss_G: 1.6165 D(x): 0.9452 D(G(z)): 0.2416 / 0.1826\n",
            "[0/25][107/600] Loss_VAE: 10.2299 Loss_D: 0.7825 Loss_G: 1.7496 D(x): 0.9607 D(G(z)): 0.1784 / 0.2227\n",
            "[0/25][108/600] Loss_VAE: 10.2849 Loss_D: 1.3189 Loss_G: 1.1190 D(x): 0.9728 D(G(z)): 0.2429 / 0.2756\n",
            "[0/25][109/600] Loss_VAE: 10.9007 Loss_D: 0.1322 Loss_G: 1.4552 D(x): 0.9687 D(G(z)): 0.2703 / 0.1617\n",
            "[0/25][110/600] Loss_VAE: 11.8327 Loss_D: 0.7637 Loss_G: 1.7368 D(x): 0.9624 D(G(z)): 0.1604 / 0.1166\n",
            "[0/25][111/600] Loss_VAE: 12.7069 Loss_D: 0.3006 Loss_G: 1.8005 D(x): 0.9638 D(G(z)): 0.1219 / 0.1750\n",
            "[0/25][112/600] Loss_VAE: 13.6737 Loss_D: 1.0579 Loss_G: 1.4325 D(x): 0.9652 D(G(z)): 0.1772 / 0.1385\n",
            "[0/25][113/600] Loss_VAE: 14.3438 Loss_D: -0.2377 Loss_G: 2.5315 D(x): 0.9631 D(G(z)): 0.1468 / 0.1090\n",
            "[0/25][114/600] Loss_VAE: 14.7517 Loss_D: 1.0058 Loss_G: 2.6682 D(x): 0.9641 D(G(z)): 0.1162 / 0.0894\n",
            "[0/25][115/600] Loss_VAE: 13.8297 Loss_D: 1.2888 Loss_G: 2.6411 D(x): 0.9441 D(G(z)): 0.0886 / 0.0614\n",
            "[0/25][116/600] Loss_VAE: 13.3887 Loss_D: 1.0987 Loss_G: 2.6943 D(x): 0.8821 D(G(z)): 0.0668 / 0.1011\n",
            "[0/25][117/600] Loss_VAE: 12.3126 Loss_D: 0.7763 Loss_G: 1.9763 D(x): 0.8534 D(G(z)): 0.1066 / 0.1460\n",
            "[0/25][118/600] Loss_VAE: 11.3071 Loss_D: 0.7760 Loss_G: 2.3494 D(x): 0.8427 D(G(z)): 0.1534 / 0.1045\n",
            "[0/25][119/600] Loss_VAE: 10.6881 Loss_D: 0.7391 Loss_G: 2.9760 D(x): 0.7406 D(G(z)): 0.1109 / 0.0795\n",
            "[0/25][120/600] Loss_VAE: 9.7808 Loss_D: 0.9407 Loss_G: 2.0905 D(x): 0.7292 D(G(z)): 0.0837 / 0.0836\n",
            "[0/25][121/600] Loss_VAE: 8.7315 Loss_D: 0.4459 Loss_G: 1.4836 D(x): 0.6865 D(G(z)): 0.0820 / 0.1128\n",
            "[0/25][122/600] Loss_VAE: 8.4325 Loss_D: -0.0065 Loss_G: 1.6951 D(x): 0.9058 D(G(z)): 0.1133 / 0.1575\n",
            "[0/25][123/600] Loss_VAE: 7.9826 Loss_D: 1.6343 Loss_G: 1.1107 D(x): 0.9644 D(G(z)): 0.1706 / 0.2265\n",
            "[0/25][124/600] Loss_VAE: 7.9460 Loss_D: 0.0248 Loss_G: 0.9380 D(x): 0.9640 D(G(z)): 0.2377 / 0.2886\n",
            "[0/25][125/600] Loss_VAE: 8.3999 Loss_D: 0.2975 Loss_G: 1.2001 D(x): 0.9730 D(G(z)): 0.2957 / 0.2049\n",
            "[0/25][126/600] Loss_VAE: 8.2579 Loss_D: 1.7507 Loss_G: 1.2997 D(x): 0.9790 D(G(z)): 0.2016 / 0.1765\n",
            "[0/25][127/600] Loss_VAE: 8.0979 Loss_D: 1.7075 Loss_G: 2.5843 D(x): 0.9601 D(G(z)): 0.1807 / 0.1013\n",
            "[0/25][128/600] Loss_VAE: 9.0669 Loss_D: 0.4178 Loss_G: 2.2660 D(x): 0.8901 D(G(z)): 0.1040 / 0.1164\n",
            "[0/25][129/600] Loss_VAE: 9.7795 Loss_D: 0.9950 Loss_G: 2.7146 D(x): 0.8894 D(G(z)): 0.1226 / 0.0911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/25][130/600] Loss_VAE: 11.8196 Loss_D: 0.8225 Loss_G: 2.1035 D(x): 0.7993 D(G(z)): 0.0905 / 0.1193\n",
            "[0/25][131/600] Loss_VAE: 14.4376 Loss_D: 0.8935 Loss_G: 2.7731 D(x): 0.8285 D(G(z)): 0.1159 / 0.0893\n",
            "[0/25][132/600] Loss_VAE: 15.9844 Loss_D: 0.2972 Loss_G: 2.4584 D(x): 0.7809 D(G(z)): 0.0895 / 0.1027\n",
            "[0/25][133/600] Loss_VAE: 16.1199 Loss_D: 0.5836 Loss_G: 1.4292 D(x): 0.8598 D(G(z)): 0.1017 / 0.2146\n",
            "[0/25][134/600] Loss_VAE: 13.8094 Loss_D: 0.8250 Loss_G: 1.3990 D(x): 0.9248 D(G(z)): 0.2225 / 0.2401\n",
            "[0/25][135/600] Loss_VAE: 10.9623 Loss_D: 0.1885 Loss_G: 0.9480 D(x): 0.9516 D(G(z)): 0.2463 / 0.2648\n",
            "[0/25][136/600] Loss_VAE: 10.0425 Loss_D: 0.3689 Loss_G: 0.9067 D(x): 0.9676 D(G(z)): 0.2642 / 0.3478\n",
            "[0/25][137/600] Loss_VAE: 9.0631 Loss_D: 0.8753 Loss_G: 1.5323 D(x): 0.9822 D(G(z)): 0.3569 / 0.2160\n",
            "[0/25][138/600] Loss_VAE: 9.0357 Loss_D: 0.2564 Loss_G: 2.3168 D(x): 0.9816 D(G(z)): 0.2206 / 0.0828\n",
            "[0/25][139/600] Loss_VAE: 8.9833 Loss_D: -0.1856 Loss_G: 2.9721 D(x): 0.9728 D(G(z)): 0.0830 / 0.0613\n",
            "[0/25][140/600] Loss_VAE: 8.4340 Loss_D: -0.0702 Loss_G: 3.2761 D(x): 0.9780 D(G(z)): 0.0644 / 0.0510\n",
            "[0/25][141/600] Loss_VAE: 8.1809 Loss_D: 0.0213 Loss_G: 1.9354 D(x): 0.9828 D(G(z)): 0.0603 / 0.0601\n",
            "[0/25][142/600] Loss_VAE: 7.7673 Loss_D: -0.2439 Loss_G: 2.9798 D(x): 0.9848 D(G(z)): 0.0650 / 0.0647\n",
            "[0/25][143/600] Loss_VAE: 7.5582 Loss_D: 0.9697 Loss_G: 1.2255 D(x): 0.9909 D(G(z)): 0.0658 / 0.2158\n",
            "[0/25][144/600] Loss_VAE: 8.0692 Loss_D: 1.9397 Loss_G: 1.6592 D(x): 0.9944 D(G(z)): 0.2294 / 0.0933\n",
            "[0/25][145/600] Loss_VAE: 8.5041 Loss_D: 1.1622 Loss_G: 2.1043 D(x): 0.9879 D(G(z)): 0.0987 / 0.0504\n",
            "[0/25][146/600] Loss_VAE: 10.0539 Loss_D: 1.1913 Loss_G: 2.9728 D(x): 0.9593 D(G(z)): 0.0505 / 0.0634\n",
            "[0/25][147/600] Loss_VAE: 12.0057 Loss_D: 1.3907 Loss_G: 2.1505 D(x): 0.9175 D(G(z)): 0.0692 / 0.0650\n",
            "[0/25][148/600] Loss_VAE: 15.4367 Loss_D: 1.0774 Loss_G: 3.0795 D(x): 0.6529 D(G(z)): 0.0667 / 0.0700\n",
            "[0/25][149/600] Loss_VAE: 19.0603 Loss_D: 1.3349 Loss_G: 1.6348 D(x): 0.5036 D(G(z)): 0.0753 / 0.2167\n",
            "[0/25][150/600] Loss_VAE: 20.4589 Loss_D: 0.2359 Loss_G: 1.2659 D(x): 0.8731 D(G(z)): 0.2255 / 0.3118\n",
            "[0/25][151/600] Loss_VAE: 17.5834 Loss_D: 0.5842 Loss_G: 0.9327 D(x): 0.9559 D(G(z)): 0.3148 / 0.3118\n",
            "[0/25][152/600] Loss_VAE: 13.3096 Loss_D: 1.1011 Loss_G: 1.4478 D(x): 0.9746 D(G(z)): 0.3319 / 0.2542\n",
            "[0/25][153/600] Loss_VAE: 9.9263 Loss_D: 1.3210 Loss_G: 1.0712 D(x): 0.9661 D(G(z)): 0.2800 / 0.2256\n",
            "[0/25][154/600] Loss_VAE: 8.1456 Loss_D: 1.4274 Loss_G: 2.2377 D(x): 0.9605 D(G(z)): 0.2412 / 0.1113\n",
            "[0/25][155/600] Loss_VAE: 7.9378 Loss_D: 1.0400 Loss_G: 2.6781 D(x): 0.8836 D(G(z)): 0.1160 / 0.0905\n",
            "[0/25][156/600] Loss_VAE: 8.1490 Loss_D: 1.2356 Loss_G: 1.4402 D(x): 0.8285 D(G(z)): 0.0960 / 0.1311\n",
            "[0/25][157/600] Loss_VAE: 9.5401 Loss_D: 0.4826 Loss_G: 1.5742 D(x): 0.7988 D(G(z)): 0.1251 / 0.1860\n",
            "[0/25][158/600] Loss_VAE: 10.5227 Loss_D: 1.2248 Loss_G: 1.7921 D(x): 0.8630 D(G(z)): 0.1900 / 0.2061\n",
            "[0/25][159/600] Loss_VAE: 11.9878 Loss_D: 0.3847 Loss_G: 1.6878 D(x): 0.8458 D(G(z)): 0.2117 / 0.2210\n",
            "[0/25][160/600] Loss_VAE: 13.1907 Loss_D: 0.9353 Loss_G: 1.7646 D(x): 0.8912 D(G(z)): 0.2191 / 0.1416\n",
            "[0/25][161/600] Loss_VAE: 13.8560 Loss_D: 0.4162 Loss_G: 1.9857 D(x): 0.8347 D(G(z)): 0.1342 / 0.0967\n",
            "[0/25][162/600] Loss_VAE: 14.8168 Loss_D: 1.5221 Loss_G: 1.9901 D(x): 0.8575 D(G(z)): 0.1044 / 0.1153\n",
            "[0/25][163/600] Loss_VAE: 14.4505 Loss_D: 0.8416 Loss_G: 2.1731 D(x): 0.8522 D(G(z)): 0.1180 / 0.1536\n",
            "[0/25][164/600] Loss_VAE: 12.2949 Loss_D: 1.4132 Loss_G: 1.3002 D(x): 0.8516 D(G(z)): 0.1547 / 0.1957\n",
            "[0/25][165/600] Loss_VAE: 9.6976 Loss_D: 0.4576 Loss_G: 1.2563 D(x): 0.8168 D(G(z)): 0.1885 / 0.1861\n",
            "[0/25][166/600] Loss_VAE: 8.0917 Loss_D: 0.7368 Loss_G: 1.7816 D(x): 0.8532 D(G(z)): 0.1963 / 0.1315\n",
            "[0/25][167/600] Loss_VAE: 6.8236 Loss_D: 0.9680 Loss_G: 1.8156 D(x): 0.8046 D(G(z)): 0.1297 / 0.1130\n",
            "[0/25][168/600] Loss_VAE: 6.3112 Loss_D: 0.6241 Loss_G: 1.9020 D(x): 0.7576 D(G(z)): 0.1146 / 0.1645\n",
            "[0/25][169/600] Loss_VAE: 5.9266 Loss_D: 1.4555 Loss_G: 1.8340 D(x): 0.8844 D(G(z)): 0.1740 / 0.1875\n",
            "[0/25][170/600] Loss_VAE: 5.9891 Loss_D: 0.8558 Loss_G: 2.0658 D(x): 0.8592 D(G(z)): 0.2003 / 0.1470\n",
            "[0/25][171/600] Loss_VAE: 6.1356 Loss_D: 0.5377 Loss_G: 1.5603 D(x): 0.8453 D(G(z)): 0.1523 / 0.1999\n",
            "[0/25][172/600] Loss_VAE: 6.0212 Loss_D: 0.9783 Loss_G: 1.8447 D(x): 0.8631 D(G(z)): 0.2134 / 0.1925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/25][173/600] Loss_VAE: 5.7290 Loss_D: 1.0707 Loss_G: 2.0270 D(x): 0.8875 D(G(z)): 0.1991 / 0.1288\n",
            "[0/25][174/600] Loss_VAE: 6.2417 Loss_D: 0.5054 Loss_G: 1.7249 D(x): 0.8151 D(G(z)): 0.1404 / 0.1282\n",
            "[0/25][175/600] Loss_VAE: 6.3024 Loss_D: 0.8946 Loss_G: 2.4024 D(x): 0.8416 D(G(z)): 0.1387 / 0.1319\n",
            "[0/25][176/600] Loss_VAE: 6.5499 Loss_D: 1.5000 Loss_G: 1.9963 D(x): 0.8656 D(G(z)): 0.1380 / 0.1424\n",
            "[0/25][177/600] Loss_VAE: 7.0755 Loss_D: 0.3415 Loss_G: 1.8854 D(x): 0.8078 D(G(z)): 0.1529 / 0.1526\n",
            "[0/25][178/600] Loss_VAE: 7.3964 Loss_D: 0.7413 Loss_G: 2.0748 D(x): 0.8584 D(G(z)): 0.1594 / 0.1104\n",
            "[0/25][179/600] Loss_VAE: 8.0782 Loss_D: 1.4793 Loss_G: 2.4599 D(x): 0.8092 D(G(z)): 0.1071 / 0.1152\n",
            "[0/25][180/600] Loss_VAE: 8.8513 Loss_D: 0.3844 Loss_G: 1.2551 D(x): 0.7717 D(G(z)): 0.1238 / 0.1489\n",
            "[0/25][181/600] Loss_VAE: 9.6621 Loss_D: 1.1522 Loss_G: 2.1953 D(x): 0.8165 D(G(z)): 0.1550 / 0.1110\n",
            "[0/25][182/600] Loss_VAE: 10.1755 Loss_D: 1.3864 Loss_G: 2.0664 D(x): 0.7786 D(G(z)): 0.1278 / 0.1400\n",
            "[0/25][183/600] Loss_VAE: 11.6832 Loss_D: 1.2843 Loss_G: 1.3995 D(x): 0.7381 D(G(z)): 0.1410 / 0.2341\n",
            "[0/25][184/600] Loss_VAE: 12.4196 Loss_D: 1.2044 Loss_G: 1.4206 D(x): 0.6763 D(G(z)): 0.2491 / 0.2797\n",
            "[0/25][185/600] Loss_VAE: 12.5719 Loss_D: 0.9723 Loss_G: 0.9147 D(x): 0.6531 D(G(z)): 0.3139 / 0.3978\n",
            "[0/25][186/600] Loss_VAE: 12.8175 Loss_D: 1.3032 Loss_G: 3.0526 D(x): 0.7625 D(G(z)): 0.4300 / 0.0318\n",
            "[0/25][187/600] Loss_VAE: 12.5385 Loss_D: 0.5149 Loss_G: 2.4384 D(x): 0.6669 D(G(z)): 0.0425 / 0.0346\n",
            "[0/25][188/600] Loss_VAE: 11.1354 Loss_D: 1.1276 Loss_G: 2.2327 D(x): 0.7579 D(G(z)): 0.0450 / 0.0610\n",
            "[0/25][189/600] Loss_VAE: 9.6825 Loss_D: 0.5992 Loss_G: 2.3691 D(x): 0.7549 D(G(z)): 0.0776 / 0.1036\n",
            "[0/25][190/600] Loss_VAE: 9.5968 Loss_D: 0.7013 Loss_G: 1.8284 D(x): 0.8604 D(G(z)): 0.1177 / 0.1741\n",
            "[0/25][191/600] Loss_VAE: 9.2381 Loss_D: 0.8121 Loss_G: 1.5249 D(x): 0.9134 D(G(z)): 0.1760 / 0.1758\n",
            "[0/25][192/600] Loss_VAE: 9.8102 Loss_D: 0.7739 Loss_G: 1.0865 D(x): 0.9066 D(G(z)): 0.1779 / 0.2519\n",
            "[0/25][193/600] Loss_VAE: 10.2716 Loss_D: 0.3076 Loss_G: 1.4223 D(x): 0.9334 D(G(z)): 0.2534 / 0.2422\n",
            "[0/25][194/600] Loss_VAE: 11.1346 Loss_D: 1.3380 Loss_G: 1.5906 D(x): 0.9501 D(G(z)): 0.2493 / 0.1576\n",
            "[0/25][195/600] Loss_VAE: 11.6285 Loss_D: 0.5219 Loss_G: 1.5845 D(x): 0.9195 D(G(z)): 0.1685 / 0.1300\n",
            "[0/25][196/600] Loss_VAE: 10.4993 Loss_D: 0.6726 Loss_G: 1.4140 D(x): 0.9026 D(G(z)): 0.1233 / 0.1547\n",
            "[0/25][197/600] Loss_VAE: 9.2330 Loss_D: 1.1009 Loss_G: 1.4647 D(x): 0.9146 D(G(z)): 0.1535 / 0.2058\n",
            "[0/25][198/600] Loss_VAE: 8.3094 Loss_D: 0.7235 Loss_G: 1.3118 D(x): 0.9290 D(G(z)): 0.2036 / 0.2198\n",
            "[0/25][199/600] Loss_VAE: 8.0377 Loss_D: 0.0338 Loss_G: 0.9907 D(x): 0.9279 D(G(z)): 0.2195 / 0.2503\n",
            "[0/25][200/600] Loss_VAE: 8.9050 Loss_D: 0.3162 Loss_G: 0.9619 D(x): 0.9518 D(G(z)): 0.2510 / 0.2632\n",
            "[0/25][201/600] Loss_VAE: 10.9600 Loss_D: 0.5372 Loss_G: 1.2538 D(x): 0.9656 D(G(z)): 0.2595 / 0.2122\n",
            "[0/25][202/600] Loss_VAE: 14.5916 Loss_D: -0.0498 Loss_G: 1.5404 D(x): 0.9748 D(G(z)): 0.2226 / 0.2555\n",
            "[0/25][203/600] Loss_VAE: 17.2301 Loss_D: 2.0333 Loss_G: 1.3949 D(x): 0.9839 D(G(z)): 0.2681 / 0.2285\n",
            "[0/25][204/600] Loss_VAE: 17.3549 Loss_D: 0.7303 Loss_G: 1.3312 D(x): 0.9751 D(G(z)): 0.2397 / 0.1418\n",
            "[0/25][205/600] Loss_VAE: 14.9244 Loss_D: 0.7730 Loss_G: 1.2774 D(x): 0.9608 D(G(z)): 0.1402 / 0.1454\n",
            "[0/25][206/600] Loss_VAE: 12.2759 Loss_D: 0.7776 Loss_G: 1.7418 D(x): 0.9551 D(G(z)): 0.1411 / 0.1286\n",
            "[0/25][207/600] Loss_VAE: 10.5353 Loss_D: 1.1433 Loss_G: 1.9323 D(x): 0.9395 D(G(z)): 0.1235 / 0.0905\n",
            "[0/25][208/600] Loss_VAE: 9.7044 Loss_D: 0.8536 Loss_G: 2.2187 D(x): 0.8818 D(G(z)): 0.0950 / 0.0931\n",
            "[0/25][209/600] Loss_VAE: 9.9383 Loss_D: 0.3637 Loss_G: 1.3589 D(x): 0.8383 D(G(z)): 0.0946 / 0.1184\n",
            "[0/25][210/600] Loss_VAE: 9.9313 Loss_D: 1.2217 Loss_G: 2.1257 D(x): 0.9083 D(G(z)): 0.1284 / 0.1491\n",
            "[0/25][211/600] Loss_VAE: 9.1073 Loss_D: 0.8027 Loss_G: 1.9456 D(x): 0.8927 D(G(z)): 0.1435 / 0.1502\n",
            "[0/25][212/600] Loss_VAE: 8.0587 Loss_D: 0.1127 Loss_G: 1.4689 D(x): 0.8621 D(G(z)): 0.1503 / 0.1757\n",
            "[0/25][213/600] Loss_VAE: 7.8856 Loss_D: 1.2748 Loss_G: 1.5320 D(x): 0.9227 D(G(z)): 0.1685 / 0.2004\n",
            "[0/25][214/600] Loss_VAE: 7.9134 Loss_D: 1.1296 Loss_G: 1.7236 D(x): 0.9340 D(G(z)): 0.1930 / 0.2254\n",
            "[0/25][215/600] Loss_VAE: 8.1827 Loss_D: 1.3296 Loss_G: 1.3601 D(x): 0.9125 D(G(z)): 0.2297 / 0.1401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/25][216/600] Loss_VAE: 7.9726 Loss_D: 0.4023 Loss_G: 2.0434 D(x): 0.8470 D(G(z)): 0.1522 / 0.1765\n",
            "[0/25][217/600] Loss_VAE: 8.2506 Loss_D: 0.2242 Loss_G: 1.3093 D(x): 0.8842 D(G(z)): 0.1840 / 0.2219\n",
            "[0/25][218/600] Loss_VAE: 8.3333 Loss_D: 0.2160 Loss_G: 0.8283 D(x): 0.9344 D(G(z)): 0.2196 / 0.3345\n",
            "[0/25][219/600] Loss_VAE: 8.0495 Loss_D: 0.4526 Loss_G: 1.4252 D(x): 0.9662 D(G(z)): 0.3534 / 0.2728\n",
            "[0/25][220/600] Loss_VAE: 7.5179 Loss_D: -0.3246 Loss_G: 2.0845 D(x): 0.9737 D(G(z)): 0.2744 / 0.1412\n",
            "[0/25][221/600] Loss_VAE: 6.9261 Loss_D: 1.3421 Loss_G: 2.7383 D(x): 0.9780 D(G(z)): 0.1476 / 0.0761\n",
            "[0/25][222/600] Loss_VAE: 6.8564 Loss_D: 1.1367 Loss_G: 2.6137 D(x): 0.9594 D(G(z)): 0.0747 / 0.0831\n",
            "[0/25][223/600] Loss_VAE: 6.7043 Loss_D: 1.1798 Loss_G: 2.2397 D(x): 0.9547 D(G(z)): 0.0881 / 0.1045\n",
            "[0/25][224/600] Loss_VAE: 7.2035 Loss_D: 0.7451 Loss_G: 1.6327 D(x): 0.9002 D(G(z)): 0.1064 / 0.1463\n",
            "[0/25][225/600] Loss_VAE: 8.1163 Loss_D: 1.2548 Loss_G: 2.1088 D(x): 0.9216 D(G(z)): 0.1432 / 0.1528\n",
            "[0/25][226/600] Loss_VAE: 8.8358 Loss_D: 0.7727 Loss_G: 1.7484 D(x): 0.8538 D(G(z)): 0.1594 / 0.2182\n",
            "[0/25][227/600] Loss_VAE: 9.1323 Loss_D: 1.0486 Loss_G: 1.8664 D(x): 0.8999 D(G(z)): 0.2230 / 0.1948\n",
            "[0/25][228/600] Loss_VAE: 9.0817 Loss_D: 0.9833 Loss_G: 2.2678 D(x): 0.8773 D(G(z)): 0.1977 / 0.1331\n",
            "[0/25][229/600] Loss_VAE: 8.6227 Loss_D: 0.7343 Loss_G: 2.3763 D(x): 0.7613 D(G(z)): 0.1295 / 0.0860\n",
            "[0/25][230/600] Loss_VAE: 7.1343 Loss_D: 1.0361 Loss_G: 2.1412 D(x): 0.4820 D(G(z)): 0.0893 / 0.1069\n",
            "[0/25][231/600] Loss_VAE: 5.4694 Loss_D: 1.2145 Loss_G: 1.0972 D(x): 0.6666 D(G(z)): 0.1037 / 0.1942\n",
            "[0/25][232/600] Loss_VAE: 4.2874 Loss_D: 1.5217 Loss_G: 1.4149 D(x): 0.8980 D(G(z)): 0.1962 / 0.2437\n",
            "[0/25][233/600] Loss_VAE: 3.8548 Loss_D: 0.3718 Loss_G: 1.3104 D(x): 0.9023 D(G(z)): 0.2404 / 0.3185\n",
            "[0/25][234/600] Loss_VAE: 3.5596 Loss_D: 0.0854 Loss_G: 0.8284 D(x): 0.9390 D(G(z)): 0.3294 / 0.3361\n",
            "[0/25][235/600] Loss_VAE: 3.6930 Loss_D: 1.4499 Loss_G: 1.2861 D(x): 0.9628 D(G(z)): 0.3411 / 0.2770\n",
            "[0/25][236/600] Loss_VAE: 3.6320 Loss_D: -0.0280 Loss_G: 1.2018 D(x): 0.9569 D(G(z)): 0.2779 / 0.1888\n",
            "[0/25][237/600] Loss_VAE: 3.8170 Loss_D: 0.3326 Loss_G: 1.5977 D(x): 0.9599 D(G(z)): 0.1805 / 0.1116\n",
            "[0/25][238/600] Loss_VAE: 4.0940 Loss_D: 1.1174 Loss_G: 2.4750 D(x): 0.9585 D(G(z)): 0.1145 / 0.0722\n",
            "[0/25][239/600] Loss_VAE: 4.3240 Loss_D: 1.4219 Loss_G: 2.4354 D(x): 0.9204 D(G(z)): 0.0790 / 0.0633\n",
            "[0/25][240/600] Loss_VAE: 4.8199 Loss_D: 0.8869 Loss_G: 2.1584 D(x): 0.8181 D(G(z)): 0.0623 / 0.0629\n",
            "[0/25][241/600] Loss_VAE: 5.3165 Loss_D: 0.4316 Loss_G: 1.8760 D(x): 0.6998 D(G(z)): 0.0566 / 0.0730\n",
            "[0/25][242/600] Loss_VAE: 6.3077 Loss_D: 0.4852 Loss_G: 1.7583 D(x): 0.8448 D(G(z)): 0.0764 / 0.1417\n",
            "[0/25][243/600] Loss_VAE: 7.3580 Loss_D: 0.9404 Loss_G: 1.7595 D(x): 0.9410 D(G(z)): 0.1344 / 0.1586\n",
            "[0/25][244/600] Loss_VAE: 8.6280 Loss_D: 0.0836 Loss_G: 1.8031 D(x): 0.9456 D(G(z)): 0.1572 / 0.1677\n",
            "[0/25][245/600] Loss_VAE: 9.5668 Loss_D: 0.2504 Loss_G: 1.7936 D(x): 0.9546 D(G(z)): 0.1708 / 0.1298\n",
            "[0/25][246/600] Loss_VAE: 9.3415 Loss_D: 0.0429 Loss_G: 2.1199 D(x): 0.9618 D(G(z)): 0.1318 / 0.1198\n",
            "[0/25][247/600] Loss_VAE: 7.9909 Loss_D: 0.9565 Loss_G: 1.8176 D(x): 0.9706 D(G(z)): 0.1217 / 0.1017\n",
            "[0/25][248/600] Loss_VAE: 6.8604 Loss_D: 0.7266 Loss_G: 2.4335 D(x): 0.9651 D(G(z)): 0.1033 / 0.0887\n",
            "[0/25][249/600] Loss_VAE: 6.2078 Loss_D: 1.2765 Loss_G: 1.4082 D(x): 0.9587 D(G(z)): 0.0895 / 0.1171\n",
            "[0/25][250/600] Loss_VAE: 5.4812 Loss_D: 0.6932 Loss_G: 1.8452 D(x): 0.9421 D(G(z)): 0.1151 / 0.1022\n",
            "[0/25][251/600] Loss_VAE: 5.3833 Loss_D: 0.6458 Loss_G: 1.4873 D(x): 0.9311 D(G(z)): 0.0971 / 0.1067\n",
            "[0/25][252/600] Loss_VAE: 5.7501 Loss_D: 0.8534 Loss_G: 2.3171 D(x): 0.9209 D(G(z)): 0.1153 / 0.0855\n",
            "[0/25][253/600] Loss_VAE: 5.9772 Loss_D: 1.1195 Loss_G: 1.6821 D(x): 0.8514 D(G(z)): 0.0853 / 0.1106\n",
            "[0/25][254/600] Loss_VAE: 5.8707 Loss_D: 0.3817 Loss_G: 1.3403 D(x): 0.8864 D(G(z)): 0.1066 / 0.1273\n",
            "[0/25][255/600] Loss_VAE: 5.9878 Loss_D: 1.5203 Loss_G: 1.2151 D(x): 0.9007 D(G(z)): 0.1280 / 0.1608\n",
            "[0/25][256/600] Loss_VAE: 6.1609 Loss_D: 0.6208 Loss_G: 1.5174 D(x): 0.8684 D(G(z)): 0.1505 / 0.1265\n",
            "[0/25][257/600] Loss_VAE: 6.4150 Loss_D: 0.8448 Loss_G: 2.1945 D(x): 0.8299 D(G(z)): 0.1307 / 0.1107\n",
            "[0/25][258/600] Loss_VAE: 6.5852 Loss_D: 0.9757 Loss_G: 1.8925 D(x): 0.8199 D(G(z)): 0.1175 / 0.1625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/25][259/600] Loss_VAE: 7.0030 Loss_D: 0.7216 Loss_G: 1.6195 D(x): 0.8584 D(G(z)): 0.1603 / 0.2365\n",
            "[0/25][260/600] Loss_VAE: 7.1947 Loss_D: 0.9935 Loss_G: 1.3457 D(x): 0.9055 D(G(z)): 0.2400 / 0.2116\n",
            "[0/25][261/600] Loss_VAE: 6.6331 Loss_D: 0.5481 Loss_G: 1.2517 D(x): 0.9015 D(G(z)): 0.2163 / 0.1449\n",
            "[0/25][262/600] Loss_VAE: 7.1066 Loss_D: 0.2217 Loss_G: 1.5580 D(x): 0.8788 D(G(z)): 0.1412 / 0.1101\n",
            "[0/25][263/600] Loss_VAE: 7.9543 Loss_D: 0.7852 Loss_G: 2.1499 D(x): 0.8635 D(G(z)): 0.1150 / 0.1322\n",
            "[0/25][264/600] Loss_VAE: 8.4122 Loss_D: 0.4653 Loss_G: 1.3932 D(x): 0.9058 D(G(z)): 0.1450 / 0.2141\n",
            "[0/25][265/600] Loss_VAE: 7.5363 Loss_D: 1.3769 Loss_G: 1.4514 D(x): 0.9503 D(G(z)): 0.2156 / 0.2722\n",
            "[0/25][266/600] Loss_VAE: 7.5175 Loss_D: 1.0544 Loss_G: 2.2168 D(x): 0.9558 D(G(z)): 0.2859 / 0.1504\n",
            "[0/25][267/600] Loss_VAE: 7.6800 Loss_D: 0.4467 Loss_G: 1.5727 D(x): 0.9197 D(G(z)): 0.1510 / 0.0847\n",
            "[0/25][268/600] Loss_VAE: 7.6741 Loss_D: 1.0712 Loss_G: 2.7978 D(x): 0.8948 D(G(z)): 0.0861 / 0.0802\n",
            "[0/25][269/600] Loss_VAE: 7.8022 Loss_D: 0.4972 Loss_G: 1.7562 D(x): 0.8298 D(G(z)): 0.0768 / 0.0882\n",
            "[0/25][270/600] Loss_VAE: 7.7218 Loss_D: 1.0599 Loss_G: 1.7660 D(x): 0.8434 D(G(z)): 0.0810 / 0.1095\n",
            "[0/25][271/600] Loss_VAE: 7.5376 Loss_D: 0.9302 Loss_G: 2.0904 D(x): 0.8846 D(G(z)): 0.1090 / 0.1355\n",
            "[0/25][272/600] Loss_VAE: 6.9162 Loss_D: 1.0099 Loss_G: 1.1036 D(x): 0.8640 D(G(z)): 0.1476 / 0.2132\n",
            "[0/25][273/600] Loss_VAE: 6.7415 Loss_D: 0.1969 Loss_G: 1.3441 D(x): 0.8806 D(G(z)): 0.2191 / 0.2297\n",
            "[0/25][274/600] Loss_VAE: 5.9516 Loss_D: 1.1270 Loss_G: 1.7258 D(x): 0.9227 D(G(z)): 0.2377 / 0.2251\n",
            "[0/25][275/600] Loss_VAE: 5.5798 Loss_D: 0.4709 Loss_G: 0.9225 D(x): 0.9336 D(G(z)): 0.2367 / 0.2648\n",
            "[0/25][276/600] Loss_VAE: 5.1865 Loss_D: 1.5964 Loss_G: 1.4955 D(x): 0.9365 D(G(z)): 0.2723 / 0.1910\n",
            "[0/25][277/600] Loss_VAE: 4.7957 Loss_D: 0.7923 Loss_G: 2.4665 D(x): 0.9231 D(G(z)): 0.1936 / 0.1182\n",
            "[0/25][278/600] Loss_VAE: 4.6199 Loss_D: 1.5690 Loss_G: 1.5592 D(x): 0.8735 D(G(z)): 0.1194 / 0.1177\n",
            "[0/25][279/600] Loss_VAE: 4.4970 Loss_D: 0.5736 Loss_G: 1.7602 D(x): 0.7826 D(G(z)): 0.1231 / 0.1529\n",
            "[0/25][280/600] Loss_VAE: 4.8690 Loss_D: 0.8564 Loss_G: 2.2148 D(x): 0.8331 D(G(z)): 0.1593 / 0.1513\n",
            "[0/25][281/600] Loss_VAE: 5.5009 Loss_D: 0.1896 Loss_G: 1.5419 D(x): 0.8402 D(G(z)): 0.1521 / 0.1349\n",
            "[0/25][282/600] Loss_VAE: 6.1711 Loss_D: 0.4786 Loss_G: 1.6937 D(x): 0.8703 D(G(z)): 0.1319 / 0.1347\n",
            "[0/25][283/600] Loss_VAE: 6.6876 Loss_D: 0.2284 Loss_G: 1.5464 D(x): 0.9018 D(G(z)): 0.1374 / 0.1702\n",
            "[0/25][284/600] Loss_VAE: 6.8124 Loss_D: 0.4115 Loss_G: 1.8281 D(x): 0.9565 D(G(z)): 0.1751 / 0.1911\n",
            "[0/25][285/600] Loss_VAE: 6.5407 Loss_D: -0.1292 Loss_G: 1.4457 D(x): 0.9673 D(G(z)): 0.2029 / 0.1902\n",
            "[0/25][286/600] Loss_VAE: 5.8517 Loss_D: 1.5474 Loss_G: 1.5216 D(x): 0.9803 D(G(z)): 0.2083 / 0.1080\n",
            "[0/25][287/600] Loss_VAE: 5.2299 Loss_D: 1.0630 Loss_G: 2.2711 D(x): 0.9715 D(G(z)): 0.1162 / 0.0680\n",
            "[0/25][288/600] Loss_VAE: 5.2139 Loss_D: 0.7407 Loss_G: 2.3211 D(x): 0.9397 D(G(z)): 0.0703 / 0.0956\n",
            "[0/25][289/600] Loss_VAE: 5.4417 Loss_D: 1.3935 Loss_G: 1.8601 D(x): 0.9548 D(G(z)): 0.1091 / 0.1480\n",
            "[0/25][290/600] Loss_VAE: 6.1611 Loss_D: 0.0122 Loss_G: 1.2657 D(x): 0.9320 D(G(z)): 0.1523 / 0.1742\n",
            "[0/25][291/600] Loss_VAE: 6.8716 Loss_D: -0.1716 Loss_G: 1.1233 D(x): 0.9428 D(G(z)): 0.1793 / 0.1767\n",
            "[0/25][292/600] Loss_VAE: 7.5730 Loss_D: 0.7874 Loss_G: 1.8209 D(x): 0.9592 D(G(z)): 0.1695 / 0.1596\n",
            "[0/25][293/600] Loss_VAE: 7.9643 Loss_D: -0.0699 Loss_G: 1.6136 D(x): 0.9604 D(G(z)): 0.1568 / 0.1537\n",
            "[0/25][294/600] Loss_VAE: 8.4767 Loss_D: 0.6881 Loss_G: 1.8702 D(x): 0.9736 D(G(z)): 0.1464 / 0.1103\n",
            "[0/25][295/600] Loss_VAE: 8.6916 Loss_D: 1.4447 Loss_G: 1.9329 D(x): 0.9682 D(G(z)): 0.1144 / 0.0899\n",
            "[0/25][296/600] Loss_VAE: 8.5180 Loss_D: 0.2673 Loss_G: 1.6190 D(x): 0.9539 D(G(z)): 0.0867 / 0.0852\n",
            "[0/25][297/600] Loss_VAE: 8.4829 Loss_D: 1.4254 Loss_G: 1.9122 D(x): 0.9472 D(G(z)): 0.0874 / 0.0883\n",
            "[0/25][298/600] Loss_VAE: 8.3380 Loss_D: 0.5747 Loss_G: 2.2078 D(x): 0.9215 D(G(z)): 0.0875 / 0.1256\n",
            "[0/25][299/600] Loss_VAE: 7.9201 Loss_D: 0.2705 Loss_G: 2.0096 D(x): 0.9243 D(G(z)): 0.1236 / 0.1819\n",
            "[0/25][300/600] Loss_VAE: 7.5930 Loss_D: 0.4339 Loss_G: 1.6312 D(x): 0.9503 D(G(z)): 0.1831 / 0.2462\n",
            "[0/25][301/600] Loss_VAE: 6.7291 Loss_D: 1.8128 Loss_G: 1.6887 D(x): 0.9672 D(G(z)): 0.2495 / 0.2047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/25][302/600] Loss_VAE: 6.0620 Loss_D: 1.3728 Loss_G: 2.4305 D(x): 0.9461 D(G(z)): 0.2095 / 0.1294\n",
            "[0/25][303/600] Loss_VAE: 5.5230 Loss_D: 0.3828 Loss_G: 2.2792 D(x): 0.8975 D(G(z)): 0.1282 / 0.0908\n",
            "[0/25][304/600] Loss_VAE: 5.0549 Loss_D: 0.5773 Loss_G: 2.1980 D(x): 0.8453 D(G(z)): 0.0913 / 0.0777\n",
            "[0/25][305/600] Loss_VAE: 5.4772 Loss_D: 0.8470 Loss_G: 1.7428 D(x): 0.8515 D(G(z)): 0.0759 / 0.0909\n",
            "[0/25][306/600] Loss_VAE: 5.5802 Loss_D: 0.5675 Loss_G: 1.5202 D(x): 0.8488 D(G(z)): 0.0861 / 0.1436\n",
            "[0/25][307/600] Loss_VAE: 5.7244 Loss_D: 0.1512 Loss_G: 1.8024 D(x): 0.9260 D(G(z)): 0.1400 / 0.1549\n",
            "[0/25][308/600] Loss_VAE: 5.9396 Loss_D: 1.0377 Loss_G: 1.8168 D(x): 0.9322 D(G(z)): 0.1567 / 0.1260\n",
            "[0/25][309/600] Loss_VAE: 5.6828 Loss_D: 0.3620 Loss_G: 2.0349 D(x): 0.9258 D(G(z)): 0.1295 / 0.1014\n",
            "[0/25][310/600] Loss_VAE: 5.2701 Loss_D: -0.0604 Loss_G: 2.4128 D(x): 0.9363 D(G(z)): 0.1014 / 0.0967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxy3uQeOOBD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e54f97de-b42c-49c5-b362-e498e5e58584"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQs_HfRLOQbp"
      },
      "source": [
        "!ls datalab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3UXG4mTOTe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a662bdeb-c841-4330-8562-5f5761a351b7"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMS5LjurOUaM"
      },
      "source": [
        "import os \n",
        "os.mkdir('/content/output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vEmx3CGOwTH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "351fc34e-5c52-4b26-d40c-5549c1ce7c41"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CchLpSdmOy5t"
      },
      "source": [
        "!cd /content/output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRRH_n5eO3oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d0fa83b-999f-48f5-b38a-1bfdec299e5e"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4bn1U1uO84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44cd6928-d7fa-4a82-ff2a-ce371d6d7a5d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab  output\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u-I8odwO-7l"
      },
      "source": [
        "!cd output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFaTfW3GPCYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aacfa130-f4ec-4237-c469-9b4a482e43d6"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI6JgZY1PEDa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}